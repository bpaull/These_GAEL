# Descriptive Power of Tempting Model {#tempting-lab}

```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, cache = TRUE)
options(dplyr.summarise.inform = FALSE)

library(tidyverse)
library(knitr)
library(kableExtra)

source("help/0_help.R")

theme_set(theme_light())

## Global variable ====
.p_lvl <- 0
wtc_models <- c("t_wtc", "gp_wtc", "ait_wtc", "tait_wtc", "reg_wtc", 
                "ec_reg_wtc","mean_wtc", "ec_mean_wtc",
                "flexible_wtc", "all_flexible_wtc", "ec_rr_wtc")
cool_models <- c("t_wtc", "gp_wtc", "ait_wtc", 
                "ec_reg_wtc", "ec_mean_wtc")

models_to_name <- function(X) {
  m__n <- function(x) {
    if (x == "t_wtc")       return("Expected utility")
    if (x == "gp_wtc")      return("G-P temptation")
    if (x == "ait_wtc")     return("Cumulative temptation")
    if (x == "ec_reg_wtc")  return("Linear model")
    if (x == "ec_mean_wtc") return("Constant response")
    if (x == "o_wtc")       return("Observed")
    NA
  }
  
  sapply(X, m__n, USE.NAMES = FALSE)
}


```

```{r 3_amt_data_loading}
amt_data <- read_csv("data/chapter_2/recap_subject.csv")

amt_data %>% 
  filter(approvable) %>% 
  mutate(bonus = round(limit(bonus, 0.1, Inf), 2)) %>% 
  pull(bonus) -> payed_bonus

amt_data %>% 
  filter(approvable) %>% 
  pull(mturk_work_time) -> amt_time

```

```{r 3_data_loading}
menu_df <- read_rds("data/chapter_2/dataset_for_analysis.Rds")
u_estim <- read_rds("data/chapter_2/singleton_estimation.Rds")
v_estim <- read_rds("data/chapter_2/temptation_estimation.Rds")

moved_slider <- menu_df$initial_position == ifelse(menu_df$left_larger, 
                                                   menu_df$o_wtc, 
                                                   -menu_df$o_wtc)
```

```{r 3_adding_reg_var}
lMenu_binMat <- menus__binMatrice(menu_df$l_menu, prefix = "l_")
sMenu_binMat <- menus__binMatrice(menu_df$s_menu, prefix = "s_")

cbind.data.frame(menu_df, lMenu_binMat, sMenu_binMat) -> menu_df

rm(lMenu_binMat, sMenu_binMat)
```

```{r 3_reg_models_comput}
m_vars <- names(menu_df)[str_detect(names(menu_df), "[l|s]_[1-9]{1,2}")]
m_rep <- "o_wtc"
m_mod <- paste0(m_rep, " ~ ", paste0(m_vars, collapse = " + "))

aD_reg_indMod <- lapply(split(menu_df, menu_df$subject_id),
                            function(x) lm(formula = m_mod, data = x))
ecD_reg_indMod <- lapply(menu_df %>% 
                            filter(comparaison_type == "1 vs 1" |
                                   comparaison_type == "2 vs 1") %>% 
                            split(., .$subject_id),
                          function(x) lm(formula = m_mod, data = x))

ecD_reg_randIn <- lapply(menu_df %>% 
                           filter(comparaison_type == "1 vs 1" |
                                    comparaison_type == "2 vs 1") %>% 
                           group_by(subject_id) %>% 
                           mutate(across(.cols = matches("[l|s]_[1-9]{1,2}"),
                                         .fns = sample)) %>% 
                           split(., .$subject_id),
                         function(x) lm(formula = m_mod, data = x))

aD_mean_indMod <- lapply(split(menu_df, menu_df$subject_id),
                            function(x) lm(formula = "o_wtc ~ 1", data = x))

ecD_mean_indMod <- lapply(menu_df %>% 
                            filter(comparaison_type == "1 vs 1" |
                                   comparaison_type == "2 vs 1") %>% 
                            split(., .$subject_id),
                          function(x) lm(formula = "o_wtc ~ 1", data = x))

individual_data <- split(menu_df, menu_df$subject_id)
```

```{r 3_models_estimation}
menu_df %>% 
  arrange(subject_id) %>% 
  mutate(gp_wtc = mapply(GP_wtc, l_menu, s_menu, subject_id, 
                         MoreArgs = list(u_est = u_estim, v_est = v_estim),
                         USE.NAMES = FALSE),
         ait_wtc = mapply(ait_wtc, l_menu, s_menu, subject_id, 
                         MoreArgs = list(u_est = u_estim, v_est = v_estim),
                         USE.NAMES = FALSE),
         tait_wtc = mapply(tait_wtc, l_menu, s_menu, subject_id, 
                         MoreArgs = list(u_est = u_estim, v_est = v_estim),
                         USE.NAMES = FALSE),
         flexible_wtc = mapply(GP_wtc, l_menu, s_menu, subject_id, 
                         MoreArgs = list(u_est = u_estim, v_est = -v_estim),
                         USE.NAMES = FALSE),
         all_flexible_wtc = mapply(ait_wtc, l_menu, s_menu, subject_id, 
                         MoreArgs = list(u_est = u_estim, v_est = -v_estim),
                         USE.NAMES = FALSE),
         reg_wtc = sMap(predict, aD_reg_indMod, individual_data),
         ec_reg_wtc = sMap(predict, ecD_reg_indMod, individual_data),
         ec_rr_wtc = sMap(predict, ecD_reg_randIn, individual_data),
         mean_wtc = sMap(predict, aD_mean_indMod, individual_data),
         ec_mean_wtc = sMap(predict, ecD_mean_indMod, individual_data)) %>% 
  mutate(across(wtc_models, limit, -1, 1),
         full_estim = mapply(is_full_estimate, subject_id, l_menu, s_menu,
                             MoreArgs = list(estimate = v_estim),
                             USE.NAMES = FALSE)) -> all_d
```

## Introduction {#intro3}

In this chapter we present an experiment which aims to test the descriptive 
capacity of the model proposed by @gul2001temptation (hereafter G-P). 
The predictions of this model are compared with those of different economic 
models and with simple statistical models. 
To evaluate the descriptive capacity of the models, our experiment places 
subjects in a situation as close as possible to the theoretical framework of 
G-P. 
Subjects are asked to choose by means of an incentivized elicitation mechanism 
among menus composed by lotteries.
A menu here is a set of 1 to 6 lotteries, from which the subject knows that she will have to 
choose only one lottery in the end.
Our approach differs from that of the previous chapter and from experiments such 
as those of @houser2018temptation or @toussaert2018eliciting on the subject. 
Our experiment does not aim at showing the existence of a particular behavior 
predicted by the theory such as the demand for commitment or the capacity of the 
subjects to exercise their self-control. 
It aims instead to evaluate the 
ability of G-P to describe the actual behavior of the subjects, 
and to compare its performance to various alternatives, ranging from economic to 
statistical models.

This experiment does not question the existence of behavior at odds with expected utility theory (hereafter EUT), 
such as the demand for constraints (@chow2011demand, @gine2010put and @uhl2011self),  
self-control effects (@burger2011field, @mischel1989delay and 
@kuhn2014self) or more surprising attitudes like those observed in 
@dellavigna2006paying. 
These behaviors are well established in the experimental literature. 
Our experiment questions the menu preference approach as a way to rationalize 
these behaviors. 
This approach, initiated by @kreps1979representation, aims at rationalizing the 
preference that individuals may have for larger sets of choices. 
The idea being that an individual who is uncertain about his future preferences 
would prefer to have a larger number of options to choose from when making his 
decision in order to maximize his utility. 
Later G-P proposed a model in which individuals may be averse to the  
presence of certain options and therefore prefer smaller choice sets. 
G-P's model rationalizes the demand for constraints by subjects while allowing 
for the possibility of costly self-constraint. 
The G-P model was later extended to rationalize more behaviors. 
For example @gul2004self for repeated choices , @noor2010uphill for anticipated
temptation cost or @noor2015menu for more complex interactions among elements of 
a menu -- for a review of application and extensions of the original G-P model see the review by 
@lipman2013temptation.

Our experiment shows that in terms of descriptive power the G-P model is a 
slight improvement over EUT. 
But the G-P model does not do better than a dummy model where subjects all evaluate 
their menus in the same way and independently of their composition. 
Moreover, a simple regression model outperforms the G-P model.

The goal of our design is to be as close as possible to the theoretical 
framework formulated by G-P. We have therefore focused on the elicitation of 
the value of menus. 
Unfortunately this design does not allow us to to identify the  
different states of the world that are at the basis of the temptation models 
proposed by @dekel2001representing, @dekel2007representing or 
@dekel2009temptation. 
Nor can we identify the choices of different selves as in the multiple-self 
models proposed by @fudenberg2006dual.
Therefore, we will not propose any interpretation of our data in the framework 
of these models. 

Our approach will allow us to propose a method to elicit the two 
functions used in the G-P model for each of our subjects.
G-P shows that an individual whose preferences are complete, transitive, 
continuous, independent and satisfying the axiom of *Set Betweenness*: 
$$
A \succsim B \text{ implies } A \succsim A\cup B \succsim B
$$
has a utility function for menus of the form:
$$
U(A) = max_{x \in A}(u(x) + v(x)) - max_{y \in A}(v(y))
$$
Where $A$ and $B$ are menus and $u(.)$ and $v(.)$ Von-Neumann Morgenstren 
utility functions.
With $u(.)$ representing the utility for the elements taken independently 
(singletons) and $v(.)$ the temptation utility of the elements. 
What we will do in our analysis is to estimate for each menu the value 
associated for the function $u(.)$ and for the function $v(.)$ and to compare 
the theoretical value of the resulting menu to the one indicated by the subject. 
We do not test the validity of the axioms but that of their theoretical 
consequence i.e. the values predicted by the utility function corresponding to 
these axioms.

## Materials and methods {#mm3}

### Experimental design

Our experiment took place online from 12/06/2020 to 19/08/2020.
The recruitment of the subjects was done via the Amazon 
Mechanical Turk (AMT) platform. 
On the AMT platform a job offer was published indicating the approximate 
duration and the fixed payment as well as an average bonus higher than 5$. 
From this offer, subjects could accept to participate in our experiment by 
accepting the task on AMT.
No selection criteria were applied a priori to the subjects. But only the subjects having filled in a valid end-of-experiment code were included in the analysis. Conditions of minimum duration of working time and number of trials were also applied but do not exclude any subject [^321].

[^321]: Only subjects with a valid code are kept for analysis because this code allows us to pay the subjects and the author refuses to include subjects who were not paid in the study. The exhaustive inclusion criteria are listed in the pre-registration of the study available on asPredicted with the code 
[qj8mr](https://aspredicted.org/qj8mr.pdf)

The software was developed using the R language and the Shiny framework. 
Once the experiment is completed, the experimental software displays a summary 
of the subjects' bonus and a unique code to be filled in the corresponding field 
on the AMT platform. 
This code allows us to uniquely identify the subjects' responses between our 
software and the AMT platform.
The target number of subjects was 300. 
304 subjects filled in a response code on the AMT page but this code was only 
valid for 297 of them. 
The 7 subjects whose code was not valid were excluded from the dataset and were 
not paid.

The experiment proceeds 
as follows:

1. The subject faces a screen with instructions. 
This screen describes the next steps in the experiment and details how the 
subject will earn his payoff. 
The subject is given a description of the menu and how the lottery works. 
Particular emphasis is put on the impact on the bonus of the mechanism of choice 
of a menu. 
Instructions are provided in [Appendix B](#expe-instruc3).
2. The subject has to answer a short series of multiple-choice control questions. 
To continue, the subject must answer all questions correctly. 
To do so, he has as many attempts as he wants and a help is displayed for the 
questions to which the answer is wrong at the first attempt.
3. Start of incentivized learning phase:

    1. The subject has to bid on 10 pairs of menus by indicating with a slider the 
    maximum amount he is willing to pay between -1\$ and 1\$ (the bidding mechanism 
    is described in detail in the following section). 
    The menus on which the subject bids are built in the same way as those of the 
    rest of the experiment but the values used for the lotteries that compose them 
    are different.
    2. One of the bids that has just been made is drawn and the subject chooses one 
    of the lotteries that it contains.
    3. A screen indicates to the subject what amount he has won with the steps 4 and 
    5, either the amount resulting from the selected auction and the amount 
    corresponding to the resolution of the chosen lottery. 
  
6. The subject is asked to complete a second set of control questions similar to the one in step 2 
but with different questions. 
7. Main task: the subject bids on 35 comparisons between 2 menus.
8. 5 of these comparisons are drawn at random and the subject can choose one 
lottery from each of the five menus assigned to him according to his bid and the elicitation mechanism.


9. The lotteries are solved and the subject's bonus is calculated as the total 
won on the 5 bids on the selected comparisons and the results of the 5 chosen 
lotteries plus what was already won in the learning phase.
10. On the final page, a table summarizes the subject's winnings by selected 
comparisons indicating the amount won via the auction and the lottery results. 
The total of the 6 auctions and lotteries is also displayed along with a 
reminder that the subject must fill in the code displayed on this page on AMT.

Our experiment is composed of only one treatment.
The earnings of the subjects are paid via the AMT platform. 
They are composed of a fixed part of 1\$ paid to all the subjects who filled in 
a valid code in the AMT form, and a variable part calculated according to the bids on the 6 selected 
comparisons and the resolution of the 6 lotteries they chose (1 in the learning 
and 5 in the main phase).
 
Our subjects were thus paid an average of  `r round(mean(payed_bonus), 2)`$ for 
an average time of `r round(mean(amt_time/60))` minutes spent on our experiment.
So an average wage per hour of 
`r round(mean(payed_bonus)/mean(amt_time/3600), 2)`$

The objective of our experimental protocol is to elicit the value that subjects 
place on menus. 
A menu is a set of items from which the subject must choose 1 and only 1 item. 
To be as close as possible to the theoretical model of G-P we chose to build our 
menus with lotteries. 
In order to keep our experiment as simple as possible we chose to use binary 
lotteries similar to the bets that can be made in roulette games in casinos. 
That is to say lotteries allowing to win $n$\$ with a probability of $\frac{1}{n}$ 
and 0\$ otherwise. 
The menus can be composed of 1 to 6 lotteries with the possible values of 
$n \in \{2, 4, 6, 8, 16, 32\}$.
All lotteries have the same expected value of 1, and differ in variance only, 
that is increasing in $n$. 
This keeps the individual elements of the menus as simple and intuitive as 
possible.

To elicit the value of the menus we have chosen to use a variant of the BDM auction 
method proposed by @becker1964measuring. 
This auction method consists in asking the subjects the maximum amount they 
would be willing to pay to acquire a good, then to randomly choose a number; 
if the number drawn is lower than the value announced by the subject then the 
subject must buy the good for the amount drawn. 
This method encourages the subject to reveal the true maximum amount he is willing to
pay for a good. 
Indeed, a subject who does not indicate his true value runs the risk of paying 
more for a good than he is willing to pay or risks not acquiring the good for an 
amount lower than what he would have been willing to pay. 
The method we use differs from the one presented in @becker1964measuring because 
we want to compare menus between them. 
So we ask subjects for the maximum amount they is willing to pay to *exchange* one 
menu for another (we will call this amount willingness-to-change, WTC). 
To elicit the WTC between two menus we present to subjects one menu on 
the left of the screen and another on the right. Subjects have to indicate an amount 
between -\$1 and \$1 corresponding to the maximum amount they are ready to pay to 
exchange the menu on the left against the one on the right. 
If the randomly chosen number is less than the value indicated by the subject, 
the subject receives the right menu and must pay an amount equal to the number 
drawn (paying a negative amount means receiving money). 
If the number drawn at random is strictly higher than the amount indicated by 
the subject, the subject receives the left menu.

In our experiment, subjects indicate the maximum amount they are willing to pay 
using a slider located between the 2 menus. 
In order not to induce an anchoring effect, the initial position of the slider 
is determined randomly for each comparison. 
This allows us to see that in `r round(mean(!moved_slider) * 100, 2)`% of the 
cases the subjects have effectively indicated their preference by moving the 
slider from the place where it was initially placed.

As with 6 different items it is possible to build 63 different menus and 
therefore 1953 different 2-by-2 comparisons, we restricted the comparisons made 
by the subjects to the ones that seemed the most interesting for us[^322]. 
These comparisons were chosen to allow reliable estimates of preferences with many 
observations of comparison of size 1 menus with each other and with size 2 
menus; 
but also to cover a wide range of items and menu sizes.
The 35 comparisons are chosen randomly and independently for each subject 
according to the following rules:

1. 10 comparisons between menus of size 1. 
The menus of size 1 are chosen randomly but in such a way that each menu appears 
at least once and that it is possible by 2-by-2 comparisons to reconstruct 1 
chain of all menus of size 1. 
This comparisons is used to estimate preferences over singletons.
2. 12 comparisons between menus of size 1 and size 2. 
The menus to be compared are drawn at random but we make sure that the menus of 
size 2 containing the elements 2 and 32 (the extreme lotteries) are compared at 
least once with the menus of {2} and {32} in order to facilitate the imputation 
by a utility function of the elements which were not directly compared.
This comparisons is used to estimate temptation preferences
3. 3 menus of size 2 between them drawn at random.
4. 3 menus of size 3 with menus of size 2 drawn at random.
5. 1 menu of size 5 drawn at random with 1 menu of size 4 and 1 menu of size 2.
6. menu of sizes 6 with 1 random menu of each other size. 

Finally, as the subjects have chosen menus and once all the comparisons are 
done, 5 comparisons among the 35 are drawn at random and a menu for each of 
the chosen comparisons is selected using the described procedure. 
The subjects then have to choose in each of these menus one and only one lottery.
This lottery is played and the outcome added to the subjects' payoffs.


[^322]: In retrospect, knowing the results of the experiment and given the problems found on value 
imputation with a utility function it would have been better to restrict the 
number of items to 4 and thus be able to explore more thoroughly the space of 2-by-2 comparisons. 


### Estimations and Models

In order to implement the economic models on our data, it is necessary to 
estimate the preferences of the subjects. In the following sections we present 
the methods we have chosen.
In order to estimate individual preferences on singletons, we use the following 
methodology:


1. We consider a menu comparison set of size 1. 
For each comparison we construct 2 equations. 
For example, if we consider the comparison between a menu A and a menu B we 
construct the 2 equations:
   * $A = B + x$
   * $B = A - x$
2. We assign a value of 0 to one of the menus.
3. We solve each equation for which the right-hand side can be calculated.
4. Each singleton is assigned the average value of the equations for which it is 
the left-hand member that could be calculated.
5. Repeat step 3 until the desired number of iterations is reached. 
6. Each singleton is assigned the average value of the last n iterations (or n 
an arbitrary number).
7. We normalize the values by subtracting the value of one of the singletons. 

We tested this methodology on our data and the procedure seem to converge 
quickly after about 30 iterations the variation in the estimated value is 
negligible.
And the result is independent of the singleton chosen as starting point.

This methodology allows us to estimate the preferences over singletons while 
taking into account the variability in the subjects' responses. 
However, this method is based on two assumptions:
there exists a preference value for the singletons to 
be estimated and the comparison of elements is symmetrical. 
We do not test either of these two hypotheses in our experiment but consider 
them as valid or at least as reasonable approximations.

To estimate the effect of temptation, we use a procedure similar to the one used 
to estimate preferences over singletons. 
The only difference is the way we build the equations from a comparison between 
two menus. 
As an example, we focus here on comparisons between menus of size 2 and size 1. 
We use the estimates made on the singletons to identify the preferred item in 
each menu of size 2. Once this item is identified we compute a theoretical 
difference with the menu of size 1 by subtracting the estimated value of the 
preferred item of the menu of size 2 from the one of size 1. 
We calculate the corrected comparison of preferences that we use to build our 2 
equations. 
For example let us consider the comparison between the menus $A = \{a_1, a_2\}$ 
and $B = \{b\}$, and suppose that the estimates of the preferences for the 
singletons yield $hat{u}(\{a_1\}) > \hat{u}(\{a_2\})$. 
We note $wtc_t = \hat{u}(\{a_1\}) - \hat{u}(\{b\})$. 
We build the 2 equations: 

* $A = B + x - wtc_t$
* $B = A - x + wtc_t$

With these equations we use the same procedure as for the estimation of 
singletons.
The preferences estimated in this way allow us to calculate the value predicted 
by the expected utility model for each of our 35 menus.

In our analysis we will compare the results of 5 different models. 
These models are grouped in two categories, the statistical models which are the 
application of statistical methods to our data and the economic models which are 
implementations of economic theories adapted to our data. 
The two statistical models are the following:

1. Constant response model. 
This model consists in calculating for each individual the aggregate average WTC he is 
willing to pay, over all comparisons between menus of size one and between menus 
of sizes two and one.
This model predicts that the WTC of an individual for any pair of menu items is 
his average aggregate WTC. 
This is a  dummy model, but it is useful as a lower-bar reference point. 
It seems reasonable to assume that a model incorporating preferences has to be
more efficient than this constant response model.
Using this kind of dummy models as a comparison point is a common practice in 
machine learning.

2. Linear regression. 
This model consists in estimating a linear regression model for each individual. 
The explained variable of this regression is the WTC between 2 menus and the 
explanatory variables are variables indicating the presence of the elements in 
the compared menus. 
This type of model is the usual starting point in machine learning, it is 
relatively simple, not expensive in terms of calculation and can easily predict 
new values if we have the corresponding explanatory variables, which is our 
case.

The economic models that we have chosen are the following:

1. The expected utility model (EUT). 
This is the model that corresponds to the standard theory in economics where 
the value of a menu depends only on the value of the item inside it that will be 
chosen, therefore on the preferred item that it contains. 
We have chosen to call it the expected utility model because our menus contain 
lotteries. 
The WTC between two menus is calculated as the difference between the preferred 
items that each menu contains. 
We have to estimate for each individual the relative value of each item 
contained in the menus.

2. The Gul and Pesendorfer model (G-P). 
In this model, we estimate the value of a menu with the following formula:
$$ U(A) = max_{x \in A}(u(x) + v(x)) - max_{y \in A}(v(y)) $$
We have to estimate for each individual two preferences for each item present in 
the menus. 
and we estimate the WTC as the difference between the estimated values of the two 
menus.

3. The cumulative temptation model. 

We estimate the value with the following formula:
$$ U(A) = max_{x \in A}(u(x) + v(x)) - \sum_{y \in A}(v(y)) $$
This model uses the same estimation as the previous one but this time the 
temptation effect of all the items of a menu is taken into account not only that
of a particular item.

## Results {#result3}

Our analysis is based on the assumption of symmetry of the 
WTC. 
Indeed, we consider that if an individual is willing to pay x\$ to switch from 
menu A to menu B, this individual will be willing to pay -x\$ to switch from 
menu B to menu A. 
As we consider this hypothesis to be true, we applied it as a pre-processing on 
our data so that the left menu is always the larger of the two menus to compare 
by modifying the WTC multiplying it by -1 when necessary. 
This modification is intended to make the analysis easier to understand and does 
not change the results.

The starting point for estimating the different economic models presented is the 
estimation of individual preferences. 
To estimate the preferences over singletons we use the 10 comparisons of menus 
of size 1 made for each individual. 
We iterate on the resulting equation system 500 times and we keep as value for 
each item the average of the last 50 iterations.
Finally as we compute relative preferences, we normalize each value by 
subtracting the value of the singleton {2}. 
This method gives us estimates for the 6 possible singletons for 
`r round(292/2.97, 2)`% of our subjects. 
An error in the programming of the experimental software has wrongly validated 
strings of singletons for `r round(5/2.97, 2)`% of the subjects making our 
estimation procedure invalid for them, so we have removed these subjects from 
our study.

Since singletons can be seen as lotteries, we can test if subjects have 
consistent preferences in terms of risks. 
We consider preferences as consistent in terms of risk if the singleton {N} is 
the singleton whose estimated utility is maximum then the estimated utility of 
the singleton {P} is higher than that of the singleton {Q} if $N > P > Q$ or if 
$N < P < Q$ i.e. single-peaked preferences. 

Among our subjects we find that 
`r round(mean(apply(u_estim, 2, valid_pref)* 100), 2)`% have consistent, 
single-peaked preferences. 
This may seem low and could call into question our method of estimating 
preferences. 
However, our estimates are strongly correlated with the subjects' responses as 
presented in the figure \@ref(fig:uEstim-plot3). 
The small number of subjects with single-peak preferences combined with the 
elements presented in the next chapter concerning the difficulties of estimating 
individual utility functions leads us not to try to smooth values in our 
estimates. 
An erroneous smoothing could be harmful to our analysis.
If this does not pose a major problem in the case of the estimation of 
singletons, we will see that it is on the other hand problematic for the 
estimation of the temptation effect.

```{r uEstim-plot3, fig.cap="High correlation between observation and estimation WTC for size 1 menus"}
all_d %>% 
  filter(comparaison_type == "1 vs 1") -> tmp

ggplot(tmp, aes(o_wtc, t_wtc)) +
  geom_jitter(width = 0.15, height = 0, alpha = 0.3, color = "#9999CC") +
  geom_abline(slope = 1, color = 'red') +
  labs(title = "Estimations predictions for size 1 menus",
       subtitle = glue::glue("correlation: {round(cor(tmp$o_wtc, 
         tmp$t_wtc), 2)}, nb obs: {nrow(tmp)} / per subject: {nrow(tmp)/
         length(unique(tmp$subject_id))}"),
       x = "observed WTC",
       y = "WTC computed with singleton utility")
```

Using the estimated utilities for the singletons and the 12 comparisons between 
size 2 and size 1 menus we can estimate the temptation utility of each item for 
each subject. 
We iterate again 500 times on the equation system provided by the 12 comparisons 
and we retain for the estimation of the relative utility of each item the 
average of the last 50 iterations.
To normalize we subtract from all values the estimate for the temptation element 
$2$.
We obtain estimates for each subject, but as expected given our design, we do not 
have the estimates for each item. 
The table \@ref(tab:no-estim3) summarizes the percentage of subjects for whom we 
have no estimate by item.

```{r no-estim3}
sub_no_v <- v_estim %>% apply(1, is.na) %>% rowSums() %>% table()

v_estim %>% apply(1, is.na) %>% colMeans() %>% t() %>% 
  (function(x) {x * 100}) %>%  
  kable(caption = "Share of subjects without estimation by items",
        digits = 2) %>% 
  kable_styling(c("striped"))
```

With `r round(sub_no_v[2]/2.92, 2)`% of the subjects for whom 1 item has no 
estimate and `r round(sub_no_v[3]/2.92, 2)`% for whom 2 items have no estimate. 
As these missing estimates are not located on the extreme values, theoretically 
it would have been easy to impute them by smoothing the estimated values with a 
utility function. 
But we have seen with the estimates of the singletons that such a smoothing is 
not reasonable in practice, as the level of consistency is too low to make this 
a meaningful exercise. 
We will therefore keep this value as missing in the rest of the analysis.
But missing values do not seem to have an 
important impact on the predictions of the G-P model. 
Indeed, by comparing the predictions made on all the comparisons between menus 
the correlation between prediction and observation is 
`r round(cor(all_d$o_wtc, all_d$gp_wtc, use = "pair"), 2)` on all the available 
value and 
`r round(cor(all_d$o_wtc[all_d$full_estim], all_d$gp_wtc[all_d$full_estim]), 2)`
when we restrict on observation for which we have all item estimated. 
Since restriction to full estimate item menus don't seem to have an important 
impact in the analysis, in the following we willuse all available data.
Figure \@ref(fig:pref-estim-plot3) shows the distribution of the estimates 
for each of the items.

```{r}
# all_d %>% 
#   filter(comparaison_type == "2 vs 1") %>% 
#   ggplot(aes(o_wtc, gp_wtc)) +
#   geom_point(alpha = 0.1) + 
#   geom_smooth(aes(color = "all comparaison"), method = 'lm', se = FALSE, alpha = 0.7) +
#   geom_smooth(data = function(x) {filter(x, full_estim)},
#               aes(color = "full estim comparaison"), method = 'lm', se = FALSE, alpha = 0.3) +
#   labs(title = "Effect of unestimate comparaison",
#        subtitle = "Comparaison of size 2 menus vs size 1",
#        x = "observed WTC",
#        y = "G-P predicted WTC",
#        color = "Used data")
# 
# all_d %>% 
#   filter(comparaison_type == "2 vs 1") -> tmp
# select(tmp, o_wtc, t_wtc, comparaison_type, gp_wtc, ait_wtc, full_estim) %>% View()
# cor(all_d$o_wtc[all_d$full_estim], all_d$gp_wtc[all_d$full_estim])
# cor(all_d$o_wtc, all_d$gp_wtc, use = "pair")
```

```{r pref-estim-plot3, fig.cap="Distribution of the estimate value of item for all subjects"}
all_estim <- u_estim %>% 
  t() %>%  
  cbind.data.frame(v_estim %>% 
                     t())
names(all_estim) <- paste0(rep(c("u_", "v_"), each = 6), names(all_estim))
all_estim %>% 
  pivot_longer(everything(), names_to = c("side", "item"), names_sep = "_") %>%
  ggplot(aes(x = item, y = value, color = side)) +
  geom_boxplot() +
  labs(title = "Item value estimation",
       color = NULL) +
  scale_color_discrete(labels = c("singleton preference", "temptation")) +
  scale_x_discrete(limits = c("2", "4", "6", "8", "16", "32"),
                   labels = c("2", "4", "6", "8", "16", "32"))
```

We can see that the estimates are of the same order of magnitude across items 
and across preferences for singletons and for the temptation effect. 
The estimates for singletons seem slightly lower than for temptation but this 
does not seem to be a significant difference. 
There are no elements that appear to be outliers in these estimates.
Using these estimates we can calculate for each of our economic models the WTC 
of each comparison made by our subjects. 

As a benchmark to judge the quality of the predictions of our economic models we 
estimate two statistical models, a constant response model and a regression 
model. 
To estimate the constant response model, we simply compute the average of the 
subjects' responses in terms of WTC for the size 1 menu comparison and for the 
comparison between size 2 and size 1 menus. 
The average of these 22 comparisons for each subject is the prediction for any 
comparison made by this model. 

The regression model is estimated as follows. 
For each subject, a linear least square regression model is estimated with the 
data concerning the size 1 menu comparisons and the size 2 menu comparisons with 
size 1 menu. 
The model has the following form:

$$
WTC = \beta_0 + \beta_1 L_2 + \beta_2 L_4 + \beta_3 L_6 + \beta_4 L_8 + 
\beta_5 L_{16} + \beta_6 L_{32} + 
\beta_7 S_2 + \beta_8 S_4 + \beta_9 S_6 + \beta_{10} S_8 + \beta_{11} S_{16}
$$

Where $L_i$ is an indicator variable for the presence of item $i$ in the largest 
menu and $S_i$ the same for the presence of $i$ in the smallest menu.
Note that our model does not contain the indicator variable $S_{32}$ because it 
is a linear combination of the other variables given the menu size constraint. 
The estimates of each of the model parameters for each subject are shown in the 
figure \@ref(fig:reg-estim-plot3).

```{r}
reg_coef <- lapply(ecD_reg_indMod, `[[`, "coefficients")
reg_coef <- matrix(unlist(reg_coef), nrow = 13, 
                   dimnames = list(names(reg_coef[[1]]), names(reg_coef)))
#Distribution of the coefficent by items for all subjects
# reg-estim-plot3
```


```{r reg-estim-plot3, fig.cap="Distribution of the coefficent by items for all subjects"}
reg_coef %>%
  t() %>%
  as_tibble() %>%
  pivot_longer(everything(), names_to = c("side", "item"), names_sep = "_") %>%
  ggplot(aes(x = item, y = value, color = side)) +
  geom_boxplot() +
  labs(title = "Item value estimation with regression",
       color = NULL) +
  scale_color_discrete(labels = c("intercept", "larger menu", "smaller menu")) +
  scale_x_discrete(limits = c("2", "4", "6", "8", "16", "32", NA),
                   labels = c("2", "4", "6", "8", "16", "32", "Intercept"))
```

We can see on the graph that the effect in the different variables of the model 
are of the same order of magnitude and close to 0. 
The estimated values are close to the ones estimated for the economic models 
especially by comparing the indicator variables for the largest menu with the 
preferences for singletons and the one for the smallest menu with the estimates 
of the preferences for the temptation. However, it would be premature to 
interpret the coefficients of the regression as individual preferences for the 
different items. 
Indeed our model does not take into account the interaction between the 
different variables which could be captured in the estimates value of the 
different coefficients and bias the interpretation. 
We can however eliminate the hypothesis that subjects evaluate a menu according 
to the sum of the items that compose it. 
Indeed in this situation there would be no interaction between the different 
elements of a menu and we should observe that all the coefficients for the 
largest menu are higher or equal to 0 and all those for the smallest menu are 
lower or equal to 0. This is not the case here. 

As our objective is to use the regression model as a tool for comparison with 
other models, we will not try to interpret its coefficients at the individual 
level. 
We will simply compare the predictions generated by this model with the 
predictions of other models and the responses of the subjects.
 
Now that we have estimated the different parameters for our 5 models, we can 
compare their performances using 3 different metrics:

1. Mean Square Error (MSE), which is the mean square difference between the 
predictions of a model and the observed values. 
This metric is the default metric in many machine learning applications because 
of its mathematical form and because it penalizes models with errors that are 
very far from the observations.
2. Pearson correlation coefficient, which is an indicator of linear co-tendency 
between two sets of values. 
As our economic models are estimated from relative utility estimates, we use 
this indicator to test that a model is at least consistent with the data in 
terms of trend even if its predictions are biased.
3. Percentage of correct sign estimate: our study concerns preferences between 
menus, so we expect a model to be able to correctly predict whether an 
individual will prefer one menu to another or whether he will be indifferent 
between two menus. 
For each model we compute the number of times it predicts a WTC of the same sign 
as the one observed[^341].

[^341]: Note that the possible observations are +,- or = so it is not an exercise 
of binary prediction in which we could improve the predictions of a model by taking the 
opposite of these predictions. 

In the table \@ref(tab:models-table3), we present the results of the five models
according to the three metrics, calculated on all the comparisons.


```{r models-table3}
all_d %>% 
  select(o_wtc, cool_models) %>% 
  pivot_longer(-o_wtc, names_to = "model") %>% 
  group_by(model) %>% 
  summarise(MSE = mse(o_wtc, value),
            correlation = cor(o_wtc, value, use = "pair"),
            `correct sign` = corr_sign(o_wtc, value)) %>% 
  mutate(model = models_to_name(model)) %>% 
  arrange(MSE) %>% 
  kable(caption = "Performance of the models, all comparisons",
        digits = 2) %>% 
  kable_styling(c("striped"))

# all_d %>% 
#   filter(full_estim) %>% 
#   select(o_wtc, cool_models) %>% 
#   pivot_longer(-o_wtc, names_to = "model") %>% 
#   group_by(model) %>% 
#   summarise(MSE = mse(o_wtc, value),
#             correlation = cor(o_wtc, value, use = "pair"),
#             `correct sign` = corr_sign(o_wtc, value)) %>% 
#   mutate(model = models_to_name(model)) %>% 
#   arrange(MSE) %>% 
#   kable(caption = "All comparaison models for all estimate comparaison",
#         digits = 2) %>% 
#   kable_styling(c("striped"))
```

We see that the model that performs best by all metrics is the linear regression 
model. 
The constant response model is second in terms of percentage of correct sign and 
MSE, which tells us that the economic models are globally poor descriptors of 
the subjects' behavior. 
Among the economic models, we see that the best performing model is the G-P 
model. 
It is only beaten by the cumulative temptation model for the percentage of 
correct sign predicted. 
However, the latter model performs very poorly in terms of MSE and correlation. 
Finally, if we compare the G-P model with the expected utility model, we see 
that taking into account the temptation effect represents a marginal improvement 
in descriptive power. 
But insofar as these two models perform less well than the constant response 
model, it seems unwise to use them.

The table \@ref(tab:comp-models-table3) shows the performance of each model according to the type of 
comparison. 
Before detailing the results, it should be remembered that the different models 
were trained using the size 1 menu comparison and the size 2 menu comparison 
with the size 1 menus. 
These two categories represents `r round(22/0.35, 2)`% of the observations. 


```{r comp-models-table3}
# all_d %>% 
#   group_by(comparaison_type) %>% 
#   summarise(across(c(cool_models), ~ cor(o_wtc, .x, use = "pair"), 
#                    .names = "correlation {str_extract(.col, '.*(?=_wtc)')}"),
#             across(c(cool_models), ~ mse(o_wtc, .x), 
#                    .names = "MSE {str_extract(.col, '.*(?=_wtc)')}"),
#             across(c(cool_models), ~ corr_sign(o_wtc, .x), 
#                    .names = "correct sign {str_extract(.col, '.*(?=_wtc)')}"),
#             nb_compar = n()) -> all_compar_table # %>% 
#   # bind_rows(mutate(
#   #   summarise_if(., funs(is.numeric(.)), weighted.mean, w = .$nb_compar), 
#   #   comparaison_type = "total")) -> all_compar_table
# structure(t(all_compar_table[, -c(1, 17)]),
#           dimnames = list(names(all_compar_table)[-c(1, 17)], 
#                           all_compar_table$comparaison_type)) %>% 
#   kable(caption = "Performance of models by comparaison type",
#         digits = 2) %>% 
#   kable_styling(c("striped"))

# all_d %>% 
#   group_by(comparaison_type) %>% 
#   summarise(across(c(cool_models), ~ cor(o_wtc, .x, use = "pair"), 
#                    .names = "correlation {.col}"),
#             across(c(cool_models), ~ mse(o_wtc, .x), 
#                    .names = "MSE {.col}"),
#             across(c(cool_models), ~ corr_sign(o_wtc, .x), 
#                    .names = "correct sign {.col}"),
#             nb_compar = n()) -> all_compar_table # %>% 
#   # bind_rows(mutate(
#   #   summarise_if(., funs(is.numeric(.)), weighted.mean, w = .$nb_compar), 
#   #   comparaison_type = "total")) -> all_compar_table
# structure(t(all_compar_table[, -c(1, 17)]),
#           dimnames = list(names(all_compar_table)[-c(1, 17)], 
#                           all_compar_table$comparaison_type)) %>% 
#   as_tibble(rownames = "model") %>% 
#   mutate(model = models_to_name(str_extract(model, "[^ ]*$"))) %>% 
#   kable(caption = "Performance of models by comparaison type",
#         digits = 2) %>% 
#   kable_styling(c("striped")) %>% 
#   pack_rows(
#   index = c("correlation" = 5, "MSE" = 5, "correct sign" = 5)
# )

all_d %>% 
  mutate(comparaison_type = if_else(comparaison_type %in% c("1 vs 1", "2 vs 1"),
                                     comparaison_type,
                                     "other")) %>% 
  group_by(comparaison_type) %>% 
  summarise(across(c(cool_models), ~ cor(o_wtc, .x, use = "pair"), 
                   .names = "correlation {.col}"),
            across(c(cool_models), ~ mse(o_wtc, .x), 
                   .names = "MSE {.col}"),
            across(c(cool_models), ~ corr_sign(o_wtc, .x), 
                   .names = "correct sign {.col}"),
            nb_compar = n()) -> all_compar_table # %>% 
  # bind_rows(mutate(
  #   summarise_if(., funs(is.numeric(.)), weighted.mean, w = .$nb_compar), 
  #   comparaison_type = "total")) -> all_compar_table
structure(t(all_compar_table[, -c(1, 17)]),
          dimnames = list(names(all_compar_table)[-c(1, 17)], 
                          all_compar_table$comparaison_type)) %>% 
  as_tibble(rownames = "model") %>% # I would delete this "model" thing on top of that column, not needed (and ugly)
  mutate(model = models_to_name(str_extract(model, "[^ ]*$"))) %>% 
  kable(caption = "Performance of models by comparaison type",
        digits = 2) %>% 
  kable_styling(c("striped")) %>% 
  pack_rows(
  index = c("correlation" = 5, "MSE" = 5, "correct sign" = 5)
)
```

The first noticeable element in this table is that the different economic models 
produce the same results for comparisons between menus of size 1. 
The results for this type of comparison are quite good, they are far superior to 
those of the constant response model and only slightly inferior to the 
regression model. 
On the other hand, if we look at the comparisons of size 2 menus against size 1 
menus, the performance of the economic models falls below that of the constant 
response model. 
And this remains true for the other types of comparisons. 
The linear regression model is the best model on the training data but it too 
performs worse than the constant response model on the new comparison types, it 
remains superior to the economic model on these data. This may indicate an 
overlearning problem on the training data. 

Now if we look at the performance of the economic models we see that the G-P 
model performs better than the EUT model. 
It seems that integrating temptation in the evaluation of the comparison between 
size 2 and size 1 menus allows to improve the predictions according to our 3 
metrics. 
But this effect does not seem to have any impact for the other types of 
comparisons. 
This may indicate that the improvements for the comparison between size 2 and 
size 1 menus is due to an overlearning phenomenon. 
It may also be a sign that the G-P model does not use the right functional form. 
Indeed, alternatives to the G-P model like @noor2015menu postulate that the 
effect of temptation is not linear. 
This could be a way to improve the performance of the model for other types of 
comparisons but such a model would also be less efficient than a constant 
response model for comparisons between menus of sizes 2 and 1, which therefore 
does not seem to be a promising direction for improvement. 
Finally, the cumulative temptation model is less efficient than the G-P  model 
except in predicting the sign of the WTCs, but it is still inferior to the 
constant response model on this metric too.

We have just seen that economic models are poor descriptors of subjects' 
behavior when dealing with menus of more than one option. 
In the figure \@ref(fig:pred-plot3) we show that these models have in common to 
predict that subjects are indifferent between two menus much more frequently 
than what we observe.
And that this is also their main difference with the linear regression model 
which has better performances.

```{r pred-plot3, fig.cap="Distribution of the WTC value predict by models"}
all_d %>% 
  select(o_wtc, gp_wtc, t_wtc, ec_reg_wtc) %>% 
  pivot_longer(everything(), names_to = "models") %>% 
  mutate(line_size = if_else(models == "o_wtc", "observed", "predicted"),
         alpha_line = if_else(models == "o_wtc", "observed", "predicted"),
         models = models_to_name(models)) %>% 
  ggplot(aes(value, linetype = models, color = line_size)) +
  geom_density(alpha = 0.66) +
  scale_size_manual(values = c(2, 0.25) ) +
  labs(title = "prediction of different models",
       color = NULL,
       x = "WTC") +
  guides(size = "none")
```

If the economic models often predict the value 0 -- i.e., indifference between 
the two compared menus -- this is because they evaluate the 
menus according to the preferred item they contain. 
In the case of the expected utility model this implies that all menus that share the 
same preferred item will have the same estimated value. 
In the case of the G-P model this phenomenon is attenuated by the effect of 
temptation of another menu item but remains important and the menus that share 
their preferred item will have close values. 
However, we observe on the comparison of the menus that subjects are rarely 
indifferent between two menus.
This seems to contradict the fact that subjects evaluate menus based on a 
particular item.

```{r 3_smooth_model_plot}
# all_d %>% 
#   mutate(comparaison_type = if_else(comparaison_type == "1 vs 1",
#                                     "1 vs 1", "other")) %>% 
#   select(comparaison_type, observed = o_wtc, o_wtc = o_wtc, cool_models) %>% 
#   pivot_longer(-c(observed, comparaison_type), names_to = "models") %>% 
#   ggplot(aes(observed, value, color = models)) +
#   geom_smooth(method = "lm") +
#   facet_wrap(vars(comparaison_type))
```

```{r 3_coef_boxplot}
# reg_coef <- lapply(ecD_reg_indMod, `[[`, "coefficients")
# reg_coef <- matrix(unlist(reg_coef), nrow = 13, 
#                    dimnames = list(names(reg_coef[[1]]), names(reg_coef)))
# 
# reg_coef %>% 
#   t() %>% 
#   as_tibble() %>% 
#   pivot_longer(everything(), names_to = c("side", "item"), names_sep = "_") %>%
#   ggplot(aes(x = item, y = value, color = side)) +
#   geom_boxplot() +
#   labs(title = "Item value estimation with regression") +
#   scale_color_discrete(labels = c("intercept", "larger menu", "smaller menu")) +
#   scale_x_discrete(limits = c("2", "4", "6", "8", "16", "32", NA),
#                    labels = c("2", "4", "6", "8", "16", "32", "Intercept"))
# 
# all_estim <- u_estim %>% 
#   t() %>%  
#   cbind.data.frame(v_estim %>% 
#                      t())
# names(all_estim) <- paste0(rep(c("u_", "v_"), each = 6), names(all_estim))
# all_estim %>% 
#   pivot_longer(everything(), names_to = c("side", "item"), names_sep = "_") %>%
#   ggplot(aes(x = item, y = value, color = side)) +
#   geom_boxplot() +
#   labs(title = "Item value estimation") +
#   scale_color_discrete(labels = c("singleton preference", "temptation")) +
#   scale_x_discrete(limits = c("2", "4", "6", "8", "16", "32"),
#                    labels = c("2", "4", "6", "8", "16", "32"))
```

## Discussion {#discu3}

In this chapter we have proposed an experiment that compares the descriptive 
quality of the model in @gul2001temptation with other economic and statistical 
models.
Our test does not aim at testing the existence of a behavior predicted by the 
theory as the rest of the experimental literature on the subject but at testing 
the theoretical framework proposed to rationalize this behavior.
Our experiment thus allows us to highlight that the theoretical approach 
proposed by G-P does not adequately account for the observed behavior of the 
subjects. 
We show in effect that a dummy constant response model produces predictions as
close to the observations as G-P's model in terms of precision, tendency and 
preference between menus.

Our experiment globally questions the menu choice approach proposed by 
@kreps1979representation and followed by G-P and the temptation models presented 
in the review of @lipman2013temptation. 
These models, although an improvement over the expected utility model, are poor 
descriptors of our observations. 
Their specific item comparison-based approaches underestimate the 
difference between two menus reported by our subjects, notably by predicting 
that subjects will be indifferent between options over which subjects are not indifferent.

In this respect, machine learning methods such as simple linear regressions -- 
that take into account all elements of a menu and not just some preferred ones 
-- are at least better descriptors of our data than economic models. 
We show in effect that a simple model trained at the individual level offers 
better predictions in terms of trend predictions and preferences between menus 
than the economic models and the dummy constant response model. 

Nevertheless, it should be noted that our analysis, like the menu choice 
framework, is based on expected utility theory. 
And although we used the most robust estimation methods possible, it is possible 
that our results are biased by a discrepancy between the observed behavior of 
the subjects and that predicted by expected utility theory. 
Part of this issue will be studied in the next chapter. 

We believe that in order to propose models with better descriptive power it 
would be useful to have more information on the structure of individual errors. 
This would allow us to improve the descriptive performance of risk preference 
models as well as models derived from them such as the Gul-Pesendorfer model.
