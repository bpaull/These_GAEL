<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Repeated Choice | Two Different Experimental Approches For Testing Temptation And A Test Of Stability Of Individual Risk Preferences</title>
  <meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Repeated Choice | Two Different Experimental Approches For Testing Temptation And A Test Of Stability Of Individual Risk Preferences" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Repeated Choice | Two Different Experimental Approches For Testing Temptation And A Test Of Stability Of Individual Risk Preferences" />
  
  <meta name="twitter:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  

<meta name="author" content="Bettega Paul" />


<meta name="date" content="2021-12-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="tempting-lab.html"/>
<link rel="next" href="conclusion5.html"/>
<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Draft for Thesis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="intro1.html"><a href="intro1.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="fdj.html"><a href="fdj.html"><i class="fa fa-check"></i><b>2</b> Intra-personal conflict and self-commitment: Evidence from a sample of French gamblers</a>
<ul>
<li class="chapter" data-level="2.1" data-path="fdj.html"><a href="fdj.html#intro2"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="fdj.html"><a href="fdj.html#mm2"><i class="fa fa-check"></i><b>2.2</b> Experimental Design and Procedure</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="fdj.html"><a href="fdj.html#experimental-conditions"><i class="fa fa-check"></i><b>2.2.1</b> Experimental conditions</a></li>
<li class="chapter" data-level="2.2.2" data-path="fdj.html"><a href="fdj.html#participants"><i class="fa fa-check"></i><b>2.2.2</b> Participants</a></li>
<li class="chapter" data-level="2.2.3" data-path="fdj.html"><a href="fdj.html#control-questions"><i class="fa fa-check"></i><b>2.2.3</b> Control questions</a></li>
<li class="chapter" data-level="2.2.4" data-path="fdj.html"><a href="fdj.html#procedure"><i class="fa fa-check"></i><b>2.2.4</b> Procedure</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="fdj.html"><a href="fdj.html#result2"><i class="fa fa-check"></i><b>2.3</b> Results</a></li>
<li class="chapter" data-level="2.4" data-path="fdj.html"><a href="fdj.html#discu2"><i class="fa fa-check"></i><b>2.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="tempting-lab.html"><a href="tempting-lab.html"><i class="fa fa-check"></i><b>3</b> Descriptive Power of Tempting Model</a>
<ul>
<li class="chapter" data-level="3.1" data-path="tempting-lab.html"><a href="tempting-lab.html#intro3"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="tempting-lab.html"><a href="tempting-lab.html#mm3"><i class="fa fa-check"></i><b>3.2</b> materials and methods</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="tempting-lab.html"><a href="tempting-lab.html#experimental-design"><i class="fa fa-check"></i><b>3.2.1</b> Experimental design</a></li>
<li class="chapter" data-level="3.2.2" data-path="tempting-lab.html"><a href="tempting-lab.html#estimations-and-models"><i class="fa fa-check"></i><b>3.2.2</b> Estimations and Models</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="tempting-lab.html"><a href="tempting-lab.html#result3"><i class="fa fa-check"></i><b>3.3</b> Results</a></li>
<li class="chapter" data-level="3.4" data-path="tempting-lab.html"><a href="tempting-lab.html#discu3"><i class="fa fa-check"></i><b>3.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multi-choice.html"><a href="multi-choice.html"><i class="fa fa-check"></i><b>4</b> Repeated Choice</a>
<ul>
<li class="chapter" data-level="4.1" data-path="multi-choice.html"><a href="multi-choice.html#intro4"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="multi-choice.html"><a href="multi-choice.html#mm4"><i class="fa fa-check"></i><b>4.2</b> Experimental design</a></li>
<li class="chapter" data-level="4.3" data-path="multi-choice.html"><a href="multi-choice.html#result4"><i class="fa fa-check"></i><b>4.3</b> Results</a></li>
<li class="chapter" data-level="4.4" data-path="multi-choice.html"><a href="multi-choice.html#discu4"><i class="fa fa-check"></i><b>4.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="conclusion5.html"><a href="conclusion5.html"><i class="fa fa-check"></i><b>5</b> Conclusion</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="expe-instruc2.html"><a href="expe-instruc2.html"><i class="fa fa-check"></i><b>A</b> Experimental instruction for chapter 2</a></li>
<li class="chapter" data-level="B" data-path="expe-instruc3.html"><a href="expe-instruc3.html"><i class="fa fa-check"></i><b>B</b> Experimental instruction for chapter 3</a></li>
<li class="chapter" data-level="C" data-path="expe-instruc4.html"><a href="expe-instruc4.html"><i class="fa fa-check"></i><b>C</b> Experimental instruction for chapter 4</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Two Different Experimental Approches For Testing Temptation And A Test Of Stability Of Individual Risk Preferences</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multi-choice" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 4</span> Repeated Choice</h1>
<div id="intro4" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Introduction</h2>
<p>In this article we are interested in the following question: When an
individual is confronted several times with the same situation, does he
always make the same choice? This question has few concrete
implications. It is indeed unlikely that an individual is to make two
identical decisions in the same context. But this question seems
important from a theoretical point of view. First, for modeling, the
answer to this question is intimately linked to the choice of
static versus dynamic, deterministic versus stochastic models.
Secondly, for the analysis of results, especially in experimental
economics. Insofar as we consider that individuals always act in the
same way in the same situation, it is only necessary to have one
observation per situation. But if the behavior varies in the same
situation it may be necessary to have multiple observations.</p>
<p>In order to answer this question, we conducted an experiment. We
limited ourselves to a situation where the subjects had to choose
between different lotteries. We chose the risk preference framework for
multiple reasons. First of all, to our knowledge, the question of the
stability of preferences has not been treated in this framework, and
this is a central domain in economics. Moreover, in this field, we have
a theory that has already been widely tested on other themes and whose
extension has difficulty in responding to the contradictions highlighted
by, among others, <span class="citation"><a href="#ref-friedman2014risky" role="doc-biblioref">Friedman et al.</a> (<a href="#ref-friedman2014risky" role="doc-biblioref">2014</a>)</span>. These models are for the most part
developments of the von-Neumann and Morgenstern model of expected
utility which is a static and deterministic model. Even the stochastic
models, of which the best known is that of <span class="citation"><a href="#ref-luce2012individual" role="doc-biblioref">Luce</a> (<a href="#ref-luce2012individual" role="doc-biblioref">2012</a>)</span>, are
built around the existence of a central value for the risk preference
parameters. It seems important to us to question the static nature of
the models used. Finally, from a practical point of view, as the
question of risk preferences has already been treated in experimental
economics, we have tools to measure the parameters of risk behavior. In
our experiment, we will use a variant of the method used in
<span class="citation"><a href="#ref-lejuez2002evaluation" role="doc-biblioref">Lejuez et al.</a> (<a href="#ref-lejuez2002evaluation" role="doc-biblioref">2002</a>)</span> and <span class="citation"><a href="#ref-crosetto2013bomb" role="doc-biblioref">Crosetto and Filippin</a> (<a href="#ref-crosetto2013bomb" role="doc-biblioref">2013</a>)</span> to elicit the risk
preferences of our subjects. We also have results on similar questions
in the context of risk preferences. For example, <span class="citation"><a href="#ref-hey1994investigating" role="doc-biblioref">Hey and Orme</a> (<a href="#ref-hey1994investigating" role="doc-biblioref">1994</a>)</span>
conducted an experiment to test different developments of the expected
utility model. Even if the situations in the experiment were not the
same, this experiment shows the ability of different models to explain
individual decisions. Following the same idea, <span class="citation"><a href="#ref-wilcox2007predicting" role="doc-biblioref">Wilcox</a> (<a href="#ref-wilcox2007predicting" role="doc-biblioref">2007</a>)</span>
uses simulations and <span class="citation"><a href="#ref-hey1994investigating" role="doc-biblioref">Hey and Orme</a> (<a href="#ref-hey1994investigating" role="doc-biblioref">1994</a>)</span> data to estimate the
predictive power of the models on a new set of situations. Finally, the
<span class="citation"><a href="#ref-ert2017revisiting" role="doc-biblioref">Ert and Haruvy</a> (<a href="#ref-ert2017revisiting" role="doc-biblioref">2017</a>)</span><a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> experiment, which is interested in the learning
of subjects, provides them with feedback and thus modifies the situation
between the different elicitation of preferences, but also studies the
variation of preferences for risk between close situations.</p>
<p>Our experiment was conducted online at the end of May 2021.
The recruitment was done via Amazon Mechanical Turk and we collected data on
300 subjects as planned in our pre-registration<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a>.
The experiment itself was done on a dedicated application developed with the
Shiny framework of the R language.
It is built on an extremely simple experimental design. After making sure
that our subjects understood how the risk elicitation task works, we
measured their risk preferences 100 times.
The measurements were taken one after the other without any feedback, the
payoffs and resolution of uncertainty being calculated only at the end of the
experiment.</p>
<p>As the measurements are made in similar situations to each other, we
expect that the 100 measurements will be identical for each subject.
However, we observe that this is only the case for
6.67% . We show that only for
47.33% the answers are
normally distributed and that for at least
22% , the data does
not seem to admit a unique central value. Moreover, we show that
measures of risk preferences can lead to the wrong conclusion that a
treatment has an effect.
And that models built on a risk aversion
parameter are of poor quality both in describing the data and in
predicting the future behavior of subjects even in the simplest
situation.
But it seems possible to significantly improve the quality of
predictions by using models that take into account the past behavior of
the subjects.</p>
<p>Our experiment shows that it should not be taken for granted that when
confronted with the same situation several times an individual will
always respond in the same way.
Responses can vary significantly not only between but also within subjects.
Within-subjects variations can be so extreme
that the behavior of a subject cannot be satisfactorily summarized
by a central value.
This implies that we must pay particular attention
to the conclusion that we draw from a difference between two measures,
but also to the behavioral conclusion that we can deduce from a risk
aversion parameter.
Finally, these results must be put into perspective
by the fact that our experiment has few equivalents and that,
consequently, the results we have obtained are uncertain and must be
supported and corrected by other studies.</p>
</div>
<div id="mm4" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Experimental design</h2>
<p>Our experimental design allows us to measure 100 times the risk
preferences of each subject. Each of these measurements is performed
under conditions as close to each other as possible. Each of the
measures is performed using an elicitation mechanism regularly used for
similar purposes in economics and psychology. Each of the elicitations
is incentivized by an amount that even if relatively small in absolute
value is much higher than the average remuneration used in experimental
economics on the target population. Moreover once the amount is put in
relation to the effort and duration of the task, the corresponding
remuneration is similar or even higher than the remuneration of subjects
in laboratories. Our protocol also includes a learning period with
feedback in addition to the traditional control questions in order to
ensure that the subjects have fully understood how the elicitation
procedure works and its impact on their compensation.</p>
<p>We believe that our experimental design allows us to satisfactorily
study how individuals behave in the face of risk on a repeated basis. We
have a large number of observations per subject which allows us to
perform robust statistical analyses at the individual level. And the
observations are comparable at the individual level because we made sure
that the conditions are as close as possible in terms of incentive,
information, form and time.</p>
<p>Our experiment took place from May 25<sup>th</sup> to May 27<sup>th</sup>, 2021.
It involved 300 subjects<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a>.
The recruitment was done using Amazon Mechanical Turk and no
constraint of competence or localization was applied.
The subjects were
paid a fixed amount of 0.5$ plus a variable amount depending on the
lotteries they chose.
In order to ensure the quality of the answers provided
and rule out automatic replies by bots,
we controlled for a number of parameters on the answers provided by
our subjects:</p>
<ol style="list-style-type: decimal">
<li>A response time for the experiment of less than 10 minutes.</li>
<li>A number of trials higher than 3 for the control questions.</li>
<li>A number of changes in the answer before validation less than 112
(the minimum possible is 110) or more than 200.</li>
<li>A small variation in response time to the lottery (a standard
deviation of response time of less than 1 second).</li>
<li>A small variation in response time between responses (a standard
deviation of time less than 2 seconds).</li>
</ol>
<p>None of these criteria is in itself a sign of poor response quality, but
the combination of several of them could be problematic.
Fortunately, only 23 subjects accumulated 2 and only 2 accumulated 3 (and no
subject was flagged for 4 or more criteria).
This leads us to believe that the answers provided by the
subjects can be considered of good quality and not polluted by
automatic answer programs.</p>
<p>The course of the experiment for the subjects was as follows:</p>
<ol style="list-style-type: decimal">
<li>The subjects select the task on Amazon Mechanical Turk, they are
informed of the approximate duration as well as the amount of the
fixed payment and the nature of the task. They are presented with a
link to the experiment.</li>
<li>When they arrive at the experiment website, they are presented with a login
screen that asks them to choose a username and password (or to
provide their own if they have experienced a logout problem). If the
identifiers are valid they are secretly and randomly assigned to one of the 9
treatments.</li>
<li>The instructions of the experiment are presented to them. The instructions
explain the process of the task as well as how they will be rewarded
according to the lotteries they have chosen.
Instructions are provided in appendix <a href="expe-instruc4.html#expe-instruc4">Appendix C</a>.</li>
<li>The subjects are then asked to answer 4 simple control questions.
They have as many tries as they want to find the right answers, but
they have to succeed in order to continue.</li>
<li>The subjects of the treatments concerned must then answer a
questionnaire on risk behavior.</li>
<li>Subjects are then asked to select 10 lotteries and are shown the
bonus they would have obtained based on their choices, but they are
warned that these lotteries will not affect their bonuses. This step
serves as a learning phase so that the subjects can take the
mechanism in hand by themselves.</li>
<li>Subjects select the 100 lotteries that will be used to calculate
their bonus. This phase represents the main body of the experiment.</li>
<li>The subjects of the concerned treatments have to answer a short
questionnaire about their preference for the risks similar to the
one at the beginning of the experiment.</li>
<li>The lotteries that will be taken into account for the subjects’
bonus are drawn and uncertainty resolved in order to calculate the bonus of each subject.
The results are presented in a table with a summary of the gains as
well as the code to be filled in on Amazon Mechanical Turk so that
the subject can receive the bonus. Only the subjects correctly filling the code to AMT are paid and kept for the analysis.</li>
</ol>
<p>Our experiment includes 9 treatments, over two independent dimensions, each with
3 modalities.
The first dimension concerns the risk behavior questionnaire, that can be run at
the beginning, at the end of the experiment or both.
The second dimension concerns the lotteries proposed to the subjects.
The 3 modalities are the following:</p>
<ol style="list-style-type: decimal">
<li>Each lottery is presented on a separate screen and each of the 100
lotteries is payoff-relevant.
Each experimental currency unit is worth 0.005$.</li>
<li>The lotteries are presented 10 per screen and one lottery per screen
is randomly drawn to be payoff-relevant. Each experimental currency unit
is worth $0.05.</li>
<li>The 100 lotteries are presented all on one screen and a single lottery
is randomly drawn to be payoff-relevant. Each experimental currency unit
is worth $0.5.</li>
</ol>
<p>In each of the treatments the maximum expected payoff is $8.
The different treatments should not impact the behavior of the subjects, the
situations being theoretically similar.
Having different payment modality is a robust test against the bipolar
behaviorist bias pointed out by <span class="citation"><a href="#ref-harrison2014experimental" role="doc-biblioref">Harrison and Swarthout</a> (<a href="#ref-harrison2014experimental" role="doc-biblioref">2014</a>)</span>.
However, we cannot use any of our processing to perform robustness tests.
Our analysis of the data revealed a ghost treatment effect (detailed in the
following analysis) which may make treatments appear statistically different
without reason.
We therefore chose to conduct the analysis by pooling the treatments.</p>
<p>Incentives may seem too small, but this is not the case.
The maximum average gain for our experiment is $8.5 which
makes it an extremely well-paid task given that the average time to
complete it is 42
minutes.
This amount is also extremely dependent on the subject’s choices.
It can indeed vary from 0.5$ to 8.5$ depending on the choice,
and even in treatments where the value of a unit is 0.005$ the amount
expected from a lottery can vary from 0$ to 0.08$ which may seem low
but should be put in comparison with the time needed to make this
decision (in the order of a second) and the amount usually paid for
tasks on Amazon Mechanical Turk (e.g. <span class="citation"><a href="#ref-sjaastad2021ulyssean" role="doc-biblioref">Sjåstad and Ekström</a> (<a href="#ref-sjaastad2021ulyssean" role="doc-biblioref">2021</a>)</span> pays
subjects 0.01$ to correctly count the number of colored cells in a 150
cell matrix in 50 seconds). This leads us to believe that the collected
responses reflect, a decision similar to the
one usually observed in experimental economics when eliciting
responses from subjects in a brick-and-mortar laboratory.</p>
<p>The objective of our experimental design is to have subjects make the
same choice multiple times (or at least choices that are as close as
possible) and to incentivize them, within the framework of utility
theory, to make the same decision each time. We have therefore chosen to
repeat the same risk elicitation task 100 times in a row. The
repetitions are done one after the other without any feedback until the
end of the experiment. The different elicitations are performed in a
short time (less than 2 hours for the slowest).
The different choices do
not or can not differ between them in terms of information, time or gain
already acquired. The main difference is that as the experiment
progresses, the number of decisions made by a subject increases, but
this factor does not influence decision making in the expected utility
model (and in many other models in decision theory).</p>
<p>To ensure that subjects are encouraged to make the same decision in each
of their choices, we chose to use an elicitation method inspired by the Balloon
Analaog Risk Task (BART, <span class="citation"><a href="#ref-lejuez2002evaluation" role="doc-biblioref">Lejuez et al.</a> (<a href="#ref-lejuez2002evaluation" role="doc-biblioref">2002</a>)</span>) and the Bomb Risk Elicitation
Task (BRET <span class="citation"><a href="#ref-crosetto2013bomb" role="doc-biblioref">Crosetto and Filippin</a> (<a href="#ref-crosetto2013bomb" role="doc-biblioref">2013</a>)</span>).</p>
<p>Our task is the same as BART but without the ball.
Removing the balloon is a way to avoid the subject being distracted by the
balloon animations or being biased by his experience with real ball as
highlighted in <span class="citation"><a href="#ref-steiner2021representative" role="doc-biblioref">Steiner and Frey</a> (<a href="#ref-steiner2021representative" role="doc-biblioref">2021</a>)</span> and <span class="citation"><a href="#ref-de2020burst" role="doc-biblioref">De Groot</a> (<a href="#ref-de2020burst" role="doc-biblioref">2020</a>)</span>.
In our task subjects have to choose a value <em>n</em> between 0 and 64.
This value corresponds to a lottery which allows them to win <span class="math inline">\(n\)</span> units (whose
value depends on the treatment as explained later) with a probability of
<span class="math inline">\(\frac{64 - n}{64}\)</span> and 0 otherwise.
By considering subjects whose utility function is of the CRRA
type and using the form used by <span class="citation"><a href="#ref-wakker2008explaining" role="doc-biblioref">Wakker</a> (<a href="#ref-wakker2008explaining" role="doc-biblioref">2008</a>)</span>:
<span class="math display">\[
u(x) = 
\begin{cases} 
  x^r &amp; \text{if } r&gt;0\\
  -x^r &amp; \text{if } r&lt;0\\
  ln(x) &amp; \text{if } r=0
\end{cases}
\]</span>
We can show that this task allows to elicit the preferences of
individuals with a risk aversion parameter between 0.016 and 64. And
that for a value r between these 2 values :
<span class="math display">\[
n^* = \frac{64r}{1 + r}
\]</span>
With <span class="math inline">\(n^*\)</span> the value that maximizes the utility function <span class="math inline">\(u(\cdot)\)</span>. We can
also associate to each possible value of <span class="math inline">\(n\)</span> the parameter <span class="math inline">\(r\)</span> for which
the utility function would be maximized (except for the values 0 and 64
which are choices strictly dominated by all others):
<span class="math display">\[
r = \frac{n}{64 - n}
\]</span>
For each iteration of the task performed by a subject, we can
therefore easily associate a risk aversion parameter.</p>
</div>
<div id="result4" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Results</h2>
<p>Our analysis will be composed of 3 parts.
In the first part we will describe the individual choices in terms of variance
and associated distributions.
In doing so we will show that individual behavior is highly variable and that
they are not normally distributed as is often assumed in economic data analysis.
In the second part we will show the consequences of these specificities
and their impact on the statistical methods used to test hypotheses and
the reliability of the results. Finally, we will briefly look at a
potential way to improve the methods usually used to estimate risk
aversion.</p>
<p>Using the elicited values we will estimate different models.
We will then compare these models according to their Mean Square Error (MSE) on
subsets of our data of different sizes.
In all situations we adopt the standard practice in machine learning of training
the models on sets distinct from the test set on which we measure the MSE.
The models we use are the following:</p>
<ol style="list-style-type: decimal">
<li><strong>dummy</strong>: This model simply predicts for each subject and each
period 32.
This value is the central value among those available.
This model does not learn anything about the behavior of the
subjects and is only used as a reference to judge the performance of
the other models.
The models performing less well than this model are probably not relevant.</li>
<li><strong>mean, median, mode</strong>: These models predict for each subject the simple
mean/median/mode of the values observed during the training periods.</li>
<li><strong>r_irr</strong>: This metric is proposed to have a metric that internalizes
individual errors.
We therefore use this index <em>r of irrationality</em> which for a set of choices of a
subject indicates the value r which minimizes the irrationality in terms of
certain equivalent.
This model predicts the value corresponding to the specific risk
aversion parameter <span class="math inline">\(r\)</span> of a CRRA utility function
that minimizes for each subject its irrationality index over
the learning period.
We define irattionality index as the sum of the
difference between the certainty equivalent chosen by the subject
and the certainty equivalent of the optimal choice for a given
utility function.
<span class="math display">\[
d_{irr} = \sum_{i =1}^{i = N}c(n^*, u)-c(n_i, u) 
\]</span>
with <span class="math inline">\(c(n, u)\)</span> the certainty equivalent of the choose <span class="math inline">\(n\)</span> for
an individual with an utility function <span class="math inline">\(u\)</span>, and <span class="math inline">\(n^*\)</span> the choice that
maximize the utility function <span class="math inline">\(u\)</span>.
From this metric we estimate utility function for subject by minimising
<span class="math inline">\(d_{irr}\)</span>.
This model has the advantage of being able to be calculated for different sets
of choices and to take into account the strictly dominated choices made by a subject.</li>
<li><strong>r_local</strong>: This model predicts for each subject the value
corresponding to the average of the r-values calculated for each
training period. This model has the advantage that it can be
computed for different sets of choices like the previous one.</li>
</ol>
<p>The first notable feature of our results is the significant heterogeneity in
individual behavior. Indeed, the subjects have in the experiment
extremely different attitudes both in terms of average values chosen and
variance around this average. But even for the individuals closest to each other
in terms of mean and variance there can be significant differences in behavior
both in the frequency of variations and in their amplitudes.
Figure <a href="multi-choice.html#fig:spag-plot4">4.1</a> gives a visual glimpse of the behavioral
heterogeneity.
Each line corresponds to the choices of an individual and the individuals are
divided by increasing average choice over the rows and by increasing variation
over the columns.
Each cell of the graph contains 12 individuals.</p>
<div class="figure"><span style="display:block;" id="fig:spag-plot4"></span>
<img src="_main_files/figure-html/spag-plot4-1.png" alt="All choice. Each line show the 100 choices of a subject" width="960" />
<p class="caption">
Figure 4.1: All choice. Each line show the 100 choices of a subject
</p>
</div>
<p>This large variation in individual behavior is less evident in the
statistics.
In the table <a href="multi-choice.html#tab:sum-stat-table4">4.1</a> we have reported the
quantiles of the mean and standard deviation per subject.
In this table we have also indicated the quantiles for the 95% confidence
interval for the parameter <span class="math inline">\(r\)</span> per subject under the assumption of normal data,
as well as the part of the observations that should be outside the observed
values (inferior to 0 or superior to 64) under the assumption of a normal
distribution.</p>
<table class="table table-striped" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:sum-stat-table4">Table 4.1: </span>Statistique of individual choice
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
quantile 5%
</th>
<th style="text-align:right;">
quantile 25%
</th>
<th style="text-align:right;">
median
</th>
<th style="text-align:right;">
quantile 75%
</th>
<th style="text-align:right;">
quantile 95%
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
mean
</td>
<td style="text-align:right;">
7.56
</td>
<td style="text-align:right;">
19.06
</td>
<td style="text-align:right;">
27.64
</td>
<td style="text-align:right;">
34.47
</td>
<td style="text-align:right;">
45.06
</td>
</tr>
<tr>
<td style="text-align:left;">
sd
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
4.35
</td>
<td style="text-align:right;">
11.43
</td>
<td style="text-align:right;">
16.50
</td>
<td style="text-align:right;">
19.10
</td>
</tr>
<tr>
<td style="text-align:left;">
mean r
</td>
<td style="text-align:right;">
0.19
</td>
<td style="text-align:right;">
0.61
</td>
<td style="text-align:right;">
1.66
</td>
<td style="text-align:right;">
3.97
</td>
<td style="text-align:right;">
7.47
</td>
</tr>
<tr>
<td style="text-align:left;">
sd r
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.16
</td>
<td style="text-align:right;">
2.94
</td>
<td style="text-align:right;">
9.71
</td>
<td style="text-align:right;">
14.71
</td>
</tr>
<tr>
<td style="text-align:left;">
width of r C.I.
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.55
</td>
<td style="text-align:right;">
3.92
</td>
<td style="text-align:right;">
63.76
</td>
<td style="text-align:right;">
64.00
</td>
</tr>
<tr>
<td style="text-align:left;">
% estim not in [0,64]
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.04
</td>
<td style="text-align:right;">
4.19
</td>
<td style="text-align:right;">
7.73
</td>
<td style="text-align:right;">
16.54
</td>
</tr>
</tbody>
</table>
<p>Average results are consistent with
the literature on risk attitudes in the laboratory. The majority of the subjects
are risk averse (average <span class="math inline">\(n\)</span> lower than 32) and very few subjects have an
average higher than 48 (theoretical equivalent to an <span class="math inline">\(r\)</span> of 3). On the
other hand, with a median standard deviation of
11.43 the variability of the data is
extremely high. This variability makes us think that the behavior of the
subjects does not come down to a constant choice with a low variability
around this choice. In any case, this variability has a major impact on
the estimates of a risk aversion parameter from these data. Indeed, if we
calculate the confidence interval for the parameter <span class="math inline">\(r\)</span> per subject from
the average choice and the standard deviation observed under the
assumption of normality of the data, we obtain extremely wide intervals
as shown in the corresponding column of table.<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a></p>
<p>This high variability also makes the hypothesis that the data is
approximately normally distributed aberrant.
For at least half of the subjects
we would have under the hypothesis of normality more than
4.19% of the data which would not be
included between 0 and 64<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a>.
To finish with the hypothesis of normality of the individual choices, a
Kolmogorov-Smirnov<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a> test was carried out by individual on their choice.
For 52.67% of the subjects
the test rejects the hypothesis of normality of the data at a threshold
alpha of 5%.
We are therefore confident that a normal approximation is a poor representation
of the behavior of our subjects, especially for some of them.</p>
<p>Can the behavior of individuals be described using a central value?<br />
To test this we calculate for
each individual their mean choice 5000 times, drawing 75 out of the 100 choices
and then we test if these different values of the mean are
normally distributed. This method consists in testing the central limit
theorem on the individual bootstrapped mean. We
use 3 different normality tests to ensure the reliability of the
results obtained. The tests used here are the Kolmogorov-Smirnov
(KS), the Shapiro-Wilk (SW) and the Anderson-Darling test (AD).
Each test is based on a different criterion and we expect to have
different but consistent results. In the table <a href="multi-choice.html#tab:norm-test4">4.2</a>, we report for each
test and their different combination the percentage of subjects for whom
the test is not considered significant at the 5% alpha level.</p>
<table>
<caption>
<span id="tab:norm-test4">Table 4.2: </span>Share of subject who choice pass normality test at 5%
</caption>
<thead>
<tr>
<th style="text-align:right;">
KS
</th>
<th style="text-align:right;">
SW
</th>
<th style="text-align:right;">
AD
</th>
<th style="text-align:right;">
KS &amp; SW &amp; AD
</th>
<th style="text-align:right;">
KS or SW or AD
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
68
</td>
<td style="text-align:right;">
58.33
</td>
<td style="text-align:right;">
60.67
</td>
<td style="text-align:right;">
46.33
</td>
<td style="text-align:right;">
78
</td>
</tr>
</tbody>
</table>
<p>The normality tests show that for a significant proportion of subjects,
the mean is not normally distributed. The proportion of subjects varies
according to the test, from 32%
for the Kolmogorov-Smirnov test to
41.67% for the Shapiro-Wilk test.</p>
<p>This difference is explained by the statistics used, the
Kolmogorov-Smirnov test takes into account the maximum deviation between
the empirical distribution and the theoretical distribution while the
Shapiro-Wilk test tests the difference between the ordered values and
the observed values. We can say that the share of the subjects concerned
is between 53.67% and
22% but it is difficult to
give an exact value.</p>
<p>Beyond the fact that this confirms that individual data are not normally
distributed for a significant number of subjects, it raises an even more
important issue. Indeed, if the mean of the individual data itself is
not normally distributed, this indicates that we are in a situation
where the central limit theorem does not apply. This may have two
causes, first, our sample is too small to allow the mean to converge.
This would be problematic given that the number of data per individual
that we have is much higher than what is usually done in experimental
economics. Moreover, it would suggest that the data are distributed
according to a probability distribution for which it is necessary to
observe a large number of data to obtain a reliable estimate of the
mean, which excludes most of the distributions usually used to model
this type of data. Second, the decision process of the subjects is of a
type that does not admit first and second order moments.
This result shows us that random choice models such as <span class="citation"><a href="#ref-gul2006random" role="doc-biblioref">Gul and Pesendorfer</a> (<a href="#ref-gul2006random" role="doc-biblioref">2006</a>)</span>,
<span class="citation"><a href="#ref-gul2014random" role="doc-biblioref">Gul, Natenzon, and Pesendorfer</a> (<a href="#ref-gul2014random" role="doc-biblioref">2014</a>)</span> or <span class="citation"><a href="#ref-cerreia2019deliberately" role="doc-biblioref">Cerreia-Vioglio et al.</a> (<a href="#ref-cerreia2019deliberately" role="doc-biblioref">2019</a>)</span> are not suitable to describe the
choices of our subjects.
These models have in common that even if the choices of the subjects can vary
between 2 iterations they should be distributed around a central value in this
situation.</p>
<p>While the results reported above might seem like technicalities, we will show
that they have implications for the methods used and the results reported in
risk elicitation studies.
The first impact of the way individuals choose is on the statistical methods
used to test hypotheses in experimental economics.
The commonly used approach is to
separate the observations that will be available according to the
application or not of a treatment. We distinguish here three approaches.
The first one, which is called in between, consists in applying the
treatment to a part of the subjects and not applying it to another part,
and in comparing the two groups. The second approach, called within,
consists of applying the treatment to all subjects but on a subset of
the observations collected by subjects, and comparing the results
between the periods when the treatment was applied or not. The last one,
the difference in difference (diffDiff) approach, consists in combining
the two other approaches. The subjects are separated in two groups and
the treatment is applied only to a part of the observations of one
group, which allows to compare the differences of variations between the
two groups for the periods when the treatment is applied or not. The
comparison is then generally made using a test of equality of means
between the groups.</p>
<p>We propose here to study ghost treatment. That is to say that we randomly group
our observations as if our subjects had been subjected
to a treatment when in fact they were not.
To simulate a between-subjects treatment, the same number of subjects
were randomly drawn from each group.
To simulate a within-subjects treatment we draw a
period and we take for each subject the same number of observations
before and after this period.
To simulate a difference-in-difference
treatment we apply the same method used for within-subjects treatments but with
two randomly selected groups as done for between-subjects treatments.
For the first type, we
compare the means of the <span class="math inline">\(n\)</span> between the 2 groups with the help of a
Student’s t test. For the difference in difference simulations we
compare the difference in means between the groups for the differences
between the periods using the same test. For each of the presented
approaches we test the results with a bootstrap method on individuals
and periods.
For each of the parameters we performed 5000 draws.
For each draw we performed a test with a type I risk threshold of 5%.
In the figure <a href="multi-choice.html#fig:gt-plot4">4.2</a> we present the share of the draws for which the
differences in behavior were judged as statistically significant (note
that the ordinate axis is presented as a % of the maximum choice made by
the subjects which is 100 in between and 50 for the other methods).</p>
<div class="figure"><span style="display:block;" id="fig:gt-plot4"></span>
<img src="_main_files/figure-html/gt-plot4-1.png" alt="Bootstrap t.test at 5% levels for different design" width="672" />
<p class="caption">
Figure 4.2: Bootstrap t.test at 5% levels for different design
</p>
</div>
<p>Since the groups are randomized, it is expected that the rate of
significant cases will be equal to the type I error, that is 5%.
Especially in within as no or little individual variation is assume in the
experimental litterature.
But we notice that only the difference-in-difference method gives the expected
result.
The two other methods present a rate of significant cases much
higher than what is expected, higher than 75% when we use all the
observations we have.
This rate of significant cases seems to be little
affected by the number of subjects, in the case of the between-subjects method
the number of subjects does not even seem to have any impact On the
other hand, this rate seems to increase with the number of observations
per subject.</p>
<p>This situation is particularly problematic if we wish to
test a hypothesis on individual behavior, because taking too few
observations leads to low power and increasing the number of
observations without using adequate methods leads to a high risk of
wrongly detecting an effect.</p>
<p>Another element in data is the low correlation of risk elicitation task with
itself.
We tested the correlation of the observations between different periods.
The periods were selected using three different methods:</p>
<ol style="list-style-type: decimal">
<li><em>random</em>: the periods were randomly drawn without replacement to form
two groups of equal size.</li>
<li><em>ordered</em>: the periods were randomly drawn without replacement
before and after a value randomly drawn to form 2 groups of equal
size.</li>
<li><em>consecutive</em>: the same number of consecutive periods were drawn before and
after a randomly drawn value. This method is the closest to an
experiment consisting in testing the correlation of the elicitation
task with itself.</li>
</ol>
<p>For each of these methods we used a bootstrap method on the number of
periods and we made for each value 5000 draws. The figure <a href="multi-choice.html#fig:corr-plot4">4.3</a>
shows the evolution of the average correlation by method as a function of the
number of periods.</p>
<div class="figure"><span style="display:block;" id="fig:corr-plot4"></span>
<img src="_main_files/figure-html/corr-plot4-1.png" alt="Pearson corelation between subject choice." width="672" />
<p class="caption">
Figure 4.3: Pearson corelation between subject choice.
</p>
</div>
<p>As in theory we measure the correlation between independent and
identically distributed observations, we expect the measured correlation
to be close to 1, only subject to random sampling variations and
identical for the 3 methods.
But we observe that the measured
correlation varies between the three methods. Except for the ordered
method whose result seems to be independent of the number of periods,
the two other methods show a clear trend with the increase of the number
of periods considered. The average correlation measured by the consecutive method
is decreasing with the number of periods, while it is increasing for the
random method. Moreover, the average correlation remains relatively low
compared to what is theoretically expected. Indeed, all methods and
number of observations taken together, it is between
[0.39, 0.75]. And
for the consecutive method with 50 observations per subject it is only
0.4.
The observed correlation is therefore much lower than 1.
The risk elicitation measure is therefore less correlated with itself than one
would expect.
This must be taken into account in studies where different risk measurement
methods are compared as in <span class="citation"><a href="#ref-crosetto2016theoretical" role="doc-biblioref">Crosetto and Filippin</a> (<a href="#ref-crosetto2016theoretical" role="doc-biblioref">2016</a>)</span>.
In this kind of exercise the correlation observed between two methods can be low
compared to the theoretical value of 1 but be quite close to the correlation
that a method has with itself.
Beyond these practical considerations, this correlation of
0.4
tells us that our method of eliciting risk preferences is not reliable to
indicate that an individual is more risk averse than another;
indeed the ranking between individuals is likely to be different between 2
measures.</p>
<p>An issue related to the question of correlation is the question of the
predictability of future behavior.
This question has to our knowledge been little studied, and never on the same
set of choices as the one used for the learning of behaviors.
We will therefore compare different estimators according to the quality of their
prediction on a set of observations different from the training set.
This is cross validation, a commonly used method in machine learning.
It allows to compare models while avoiding overfitting problems.
In the figure <a href="multi-choice.html#fig:plt-error-model4">4.4</a> we show the
results of different models according to the number of periods devoted
to learning (the number of test periods and 100 minus the number of
observations devoted to learning) in terms of mean square error.</p>
<div class="figure"><span style="display:block;" id="fig:plt-error-model4"></span>
<img src="_main_files/figure-html/plt-error-model4-1.png" alt="Descriptives power of different models." width="672" />
<p class="caption">
Figure 4.4: Descriptives power of different models.
</p>
</div>
<p>We see that the models that perform the worst are the models that can be
generalized to other choice sets. The r_irr model always performs worse
than the dummy model and the r_local model performs worse as soon as the
number of observations per subject becomes larger than 10. These two
models perform poorer as the number of observations in the learning
sample increases. In general these two models perform worse than the
dummy model and do not seem to be relevant for predicting individual
behavior. The mode-based model performs a little better than the dummy
model when the number of training observations is higher than 14 but
less otherwise. The best performing models are the median- and mean-based
models. Both have a similar behavior in performing better when the
training period is long. In the end, these 2 models perform much better
than the dummy model. The mean-based model performs a little better than
the median-based model. But even with 99 training periods the mean based
model displays an MSE of more than 155.</p>
<p>In order to test the ability of the different models to predict the
future behavior of the subjects, we use a similar approach to the
previous one, but we separate the training and test samples according to
their order. We create a training sample of size m for a subject by
selecting the first m observations. Then we compute the MSE on the next
100-m observations, for each subject. By doing so we effectively test
the ability of the models to predict future behavior by respecting the
serial nature of the data. We can therefore construct a linear regression
model for each subject that includes as an explanatory variable for a
choice the choices made in previous periods. For each subject the model
includes up to the last 5 choices in order to predict the choice of the
current period, note that for some subjects the choices of the previous
periods can be perfectly correlated between them and that in this case
the number of previous choices included is reduced in order not to
include two variables perfectly correlated between them. In order to be
able to compare the performances of the different models we have used in
addition to the linear regression some of the models used previously. In
the figure <a href="multi-choice.html#fig:dummy-estim4">4.5</a> we present the performance in terms of MSE of
the different models for different learning sample sizes.</p>
<div class="figure"><span style="display:block;" id="fig:dummy-estim4"></span>
<img src="_main_files/figure-html/dummy-estim4-1.png" alt="Predictives power of different models." width="672" />
<p class="caption">
Figure 4.5: Predictives power of different models.
</p>
</div>
<p>As before, the least efficient models are those based on the calculation
of a risk aversion parameter. This type of model offers a lower
predictive power than the dummy model which always predicts 32. The
mean-based model still offers better results than the dummy model. It is
also the best performing model when the number of observations in the
training sample is low (about less than 40). Finally, the linear
regression model offers extremely variable results for a small number of
observations (about 40) but when the number of observations in the
training sample is sufficient it is this type of model that gives the
best results. Moreover, it seems that the regression model learns more
than the mean-based model when the number of observations increases,
which can lead us to hope that with a larger number of observations we
can obtain good predictions.</p>
<p>By comparing the predictions of linear and mean models, we show the
importance of a subject’s previous choices on his decisions. If linear
regressions offer better results it is because in some way the
individual choices are not independent and it is possible to build
models taking into account this dependence. Insofar as the linear
regressions we have built here were built in a rudimentary way and that
no model selection tools or even interaction terms or variables of order
different from 1 were tested, it would be possible to have better
results for this type of model. Another possibility of improvement would
be the use of other classes of models like random forests.</p>
<p>This shows that to predict individual behavior, models based on a risk
aversion parameter perform poorly on the simplest possible task. That
when it comes to summarize these predictions using a single parameter or
when we have few observation, the best parameter are the
average of observations. But that the predictions obtained in this way
are likely to be mediocre. But it should be possible to obtain better
quality predictions if we use the right models and have a large number
of observations.</p>
</div>
<div id="discu4" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Conclusion</h2>
<p>In this article we have tried to provide some answers to the following
question: When an individual is confronted several times with the same
situation, does he always make the same choice? To do this, we proposed
an experimental protocol in which our subjects were repeatedly asked the
same question. The repetitions are done one after the other and the
subjects receive no feedback before the end of the experiment. The question
asked concerns risk preferences. Risk preferences are central to many
decisions and their elicitation has already been widely studied in
experimental economics. We therefore chose to adapt the BART method used
in economics and psychology. This allowed us to repeatedly ask a
question with the following properties:</p>
<ol style="list-style-type: decimal">
<li>The answer to the question is simple (choose a number between 0 and
<ol start="64" style="list-style-type: decimal">
<li>and fast, especially since the subject chooses to always answer
in the same way.</li>
</ol></li>
<li>Subjects are encouraged to answer according to their preferences.</li>
<li>A subject with fixed preferences is encouraged to answer always in
the same way.</li>
<li>The set of possible responses is large and therefore allows for
variations in responses of different magnitudes.</li>
</ol>
<p>With this question, we expected subjects to respond in the same way on all
occasions.</p>
<p>But our results show that the subjects act in a varied way and that only
a 6.67% of them always make
the same choice. So even if the average value chosen by the subjects is
coherent with the literature on the subject, the variability of the
choices is very important between the subjects and high for an important
part of the subjects. We show that this variability in the answers has
important consequences on the modeling and estimation of the preferences
for the risk. First of all, the estimation of a risk aversion parameter
is extremely imprecise with a coefficient of variation for the parameter <span class="math inline">\(r\)</span> of
roughtly 429 for CRRA function.
The distribution of the answers is not normal for
52.67% of the subjects
and for 22% of the
subjects we cannot even be confident that their answer can be
described with a central value.</p>
<p>Beyond the theoretical considerations for modeling that our results
raise, we also show that the way individuals choose has practical
consequences both on the methods used in experimental economics to test
hypotheses and on the reliability of the conclusions that can be deduced
from the estimation of a risk aversion parameter. Indeed, our results
show that in a situation of repeated choice with between- or within-subjects
experimental designs the chances of concluding wrongly that there is a
significant effect are significantly higher than the significance threshold chosen for the tests.
The difference-in-difference designs do not seem to suffer from this
effect. We also show that the correlation of risk aversion measures for
the same task is only about
0.4.
This value is much lower than what one would expect (a correlation of 1)
and may explain the low level of correlation observed between the
various methods of measuring risk preferences. Moreover, we show that
models based on the evaluation of a risk aversion parameter that can be
generalized to different sets are very poor models for describing
individual behavior. The best alternative to describe individual
behaviors with a parameter is to use the mean observation. This
paradoxical conclusion with our first conclusion led us to test an
alternative model, individual linear regression. We compared this new
model with the others according to their predictive power. We show that
the predictive power of models based on a risk aversion parameter is as
bad as their descriptive power. But we also show that when the number of
observations is sufficient, the model based on linear regressions
provides better results than the average. This shows us that it is easy to
achieve better results than those proposed by the economic models.</p>
<p>Our results suggest that a static model based on a risk aversion
parameter is not suitable to describe individual behavior. Indeed, we
have shown that this type of model obtains poor results both from a
descriptive and predictive point of view even in the simplest task. This
is consistent with the work of <span class="citation"><a href="#ref-wilcox2007predicting" role="doc-biblioref">Wilcox</a> (<a href="#ref-wilcox2007predicting" role="doc-biblioref">2007</a>)</span> which shows the
weaknesses of this type of model in a slightly more complex task.
However, it seems possible to learn more about individual behavior by
having enough observations on repeated choices to train statistical
models that could highlight behavioral regularities.</p>
<p>Finally, we can answer our question by saying that the elements at our
disposal lead us to say that confronted with the same situation several
times, individuals will act in different ways. But this question would
deserve more work and results than the few elements we have brought
here.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-cerreia2019deliberately" class="csl-entry">
Cerreia-Vioglio, Simone, David Dillenberger, Pietro Ortoleva, and Gil Riella. 2019. <span>“Deliberately Stochastic.”</span> <em>American Economic Review</em> 109 (7): 2425–45.
</div>
<div id="ref-crosetto2013bomb" class="csl-entry">
Crosetto, Paolo, and Antonio Filippin. 2013. <span>“The <span>‘Bomb’</span> Risk Elicitation Task.”</span> <em>Journal of Risk and Uncertainty</em> 47 (1): 31–65.
</div>
<div id="ref-crosetto2016theoretical" class="csl-entry">
———. 2016. <span>“A Theoretical and Experimental Appraisal of Four Risk Elicitation Methods.”</span> <em>Experimental Economics</em> 19 (3): 613–41.
</div>
<div id="ref-de2020burst" class="csl-entry">
De Groot, Kristel. 2020. <span>“Burst Beliefs–Methodological Problems in the Balloon Analogue Risk Task and Implications for Its Use.”</span> <em>Journal of Trial and Error</em> 1 (1).
</div>
<div id="ref-ert2017revisiting" class="csl-entry">
Ert, Eyal, and Ernan Haruvy. 2017. <span>“Revisiting Risk Aversion: Can Risk Preferences Change with Experience?”</span> <em>Economics Letters</em> 151: 91–95.
</div>
<div id="ref-friedman2014risky" class="csl-entry">
Friedman, Daniel, R Mark Isaac, Duncan James, and Shyam Sunder. 2014. <em>Risky Curves: On the Empirical Failure of Expected Utility</em>. Routledge.
</div>
<div id="ref-gul2014random" class="csl-entry">
Gul, Faruk, Paulo Natenzon, and Wolfgang Pesendorfer. 2014. <span>“Random Choice as Behavioral Optimization.”</span> <em>Econometrica</em> 82 (5): 1873–1912.
</div>
<div id="ref-gul2006random" class="csl-entry">
———. 2006. <span>“Random Expected Utility.”</span> <em>Econometrica</em> 74 (1): 121–46.
</div>
<div id="ref-harrison2014experimental" class="csl-entry">
Harrison, Glenn W, and J Todd Swarthout. 2014. <span>“Experimental Payment Protocols and the Bipolar Behaviorist.”</span> <em>Theory and Decision</em> 77 (3): 423–38.
</div>
<div id="ref-hey1994investigating" class="csl-entry">
Hey, John D, and Chris Orme. 1994. <span>“Investigating Generalizations of Expected Utility Theory Using Experimental Data.”</span> <em>Econometrica: Journal of the Econometric Society</em>, 1291–1326.
</div>
<div id="ref-lejuez2002evaluation" class="csl-entry">
Lejuez, Carl W, Jennifer P Read, Christopher W Kahler, Jerry B Richards, Susan E Ramsey, Gregory L Stuart, David R Strong, and Richard A Brown. 2002. <span>“Evaluation of a Behavioral Measure of Risk Taking: The Balloon Analogue Risk Task (BART).”</span> <em>Journal of Experimental Psychology: Applied</em> 8 (2): 75.
</div>
<div id="ref-luce2012individual" class="csl-entry">
Luce, R Duncan. 2012. <em>Individual Choice Behavior: A Theoretical Analysis</em>. Courier Corporation.
</div>
<div id="ref-sjaastad2021ulyssean" class="csl-entry">
Sjåstad, Hallgeir, and Mathias Ekström. 2021. <span>“Ulyssean Self-Control: Pre-Commitment Is Effective, but Choosing It Freely Requires Good Self-Control.”</span> <em>PsyArXiv. September</em> 4.
</div>
<div id="ref-steiner2021representative" class="csl-entry">
Steiner, Markus D, and Renato Frey. 2021. <span>“Representative Design in Psychological Assessment: A Case Study Using the Balloon Analogue Risk Task (BART).”</span> <em>Journal of Experimental Psychology: General</em>.
</div>
<div id="ref-wakker2008explaining" class="csl-entry">
Wakker, Peter P. 2008. <span>“Explaining the Characteristics of the Power (CRRA) Utility Family.”</span> <em>Health Economics</em> 17 (12): 1329–44.
</div>
<div id="ref-wilcox2007predicting" class="csl-entry">
Wilcox, N. 2007. <span>“Predicting Risky Choices Out-of-Context: A Monte Carlo Study.”</span> <em>University of</em>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="9">
<li id="fn9"><p>We would like to thank Ernan Haruvy for sharing data with us. This allowed us to test our analysis on existing (albeit different) data prior to conducting our experiment.<a href="multi-choice.html#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>Pre-registered hypothesis are available with asPredicted code
<a href="https://aspredicted.org/zx667.pdf">zx667</a><a href="multi-choice.html#fnref10" class="footnote-back">↩︎</a></p></li>
<li id="fn11"><p>317 subjects finished the experiment but only those
having entered a valid payment and control code at end of experiment were taken into
account in the analysis.<a href="multi-choice.html#fnref11" class="footnote-back">↩︎</a></p></li>
<li id="fn12"><p>Note that as under the assumption of normality it is possible to
observe values of n lower than 0 (and higher than 64) the values for
the parameter r were brought back to 0 (and to 64) for the extreme
cases and that this reduces the size of the reported confidence
interval.<a href="multi-choice.html#fnref12" class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p>To claim that our results can be described with a normal distribution
would be similar to saying that a model that predicts that the height of
4.19%% of humans is less than 0 cm or greater
than 3m accurately describes reality.<a href="multi-choice.html#fnref13" class="footnote-back">↩︎</a></p></li>
<li id="fn14"><p>As normality test are sensitive to ties, and our observations are integer
and this contain ties, we add an uniform noise to observation before running the
test.<a href="multi-choice.html#fnref14" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="tempting-lab.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="conclusion5.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/4-repeated_choice.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
